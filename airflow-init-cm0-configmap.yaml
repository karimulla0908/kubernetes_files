apiVersion: v1
data:
  airflow-ingress.yaml: |
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: airflow-ingress
      namespace: airflow
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
      ingressClassName: nginx
      rules:
      - host: airflow0908.eastus.cloudapp.azure.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: airflow-webserver
                port:
                  number: 8080
  docker-compose.yaml: "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n#\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\n#\n# This configuration supports basic configuration using environment variables or an .env file\n# The following variables are supported:\n#\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n#                                Default: apache/airflow:2.6.0\n# AIRFLOW_UID                  - User ID in Airflow containers\n#                                Default: 50000\n# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n#                                Default: .\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n#\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n#                                Default: airflow\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n#                                Default: airflow\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n#                                Use this option ONLY for quick checks. Installing requirements at container\n#                                startup is done EVERY TIME the service is started.\n#                                A better way is to build a custom image or extend the official image\n#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n#                                Default: ''\n#\n# Feel free to modify this file to suit your needs.\n---\nversion: '3.8'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}\n  # build: .\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow <2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: '42oAR66FqGYr0S9IwNDUNERL78yYvRZO-tv2Y5sQE4c='\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'False'\n    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 300\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n  volumes:\n    - ./project_data:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    ports: \n    - \"5432:5432\"\n    environment:\n      POSTGRES_USER: \"airflow\"\n      POSTGRES_PASSWORD: \"airflow\"\n      POSTGRES_HOST_AUTH_METHOD: \"scram-sha-256\"\n      POSTGRES_INITDB_ARGS: \"--auth-host=scram-sha-256\"\n      POSTGRES_ROOT_PASSWORD: \"Asdo@123\"\n      POSTGRES_DB: \"airflow\"\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n    command: [\"postgres\", \"-c\", \"log_statement=ddl\"]\n\n  pgbouncer:\n    image: edoburu/pgbouncer:latest\n    environment:\n      DATABASE_URL: postgresql://airflow:airflow@postgres:5432/airflow\n      PGBOUNCER_AUTH_TYPE: md5\n      PGBOUNCER_AUTH_USER: airflow\n    ports:\n      - \"6432:6432\"\n    depends_on:\n      - postgres\n    volumes:\n      - ./pgbouncer.ini:/etc/pgbouncer/pgbouncer.ini\n      - ./userlist.txt:/etc/pgbouncer/userlist.txt\n\n\n  redis:\n    image: redis:latest\n    expose:\n      - 6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    <<: *airflow-common\n    command: celery worker\n    healthcheck:\n      test:\n        - \"CMD-SHELL\"\n        - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    environment:\n      <<: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: \"0\"\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    <<: *airflow-common\n    command: triggerer\n    healthcheck:\n      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    <<: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        function ver() {\n          printf \"%04d%04d%04d%04d\" $${1//./ }\n        }\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available < 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available < 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available < one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        mkdir -p /sources/logs /sources/dags /sources/plugins\n        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n        exec /entrypoint airflow db migrate\n        exec /entrypoint airflow db init\n        exec /entrypoint airflow version\n\n    # yamllint enable rule:line-length\n    environment:\n      <<: *airflow-common-env\n      _AIRFLOW_DB_MIGRATE: 'true'\n      _AIRFLOW_WWW_USER_CREATE: 'true'\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      AIRFLOW_PIP_ADDITIONAL_REQUIREMENTS: \"numpy pandas matplotlib azure-identity==1.12.0 azure-storage-blob==12.14.1 bs4 requests datetime logging pyarrow<10.1.0,>=10.0.1\"\n    user: \"0:0\"\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n\n  airflow-cli:\n    <<: *airflow-common\n    profiles:\n      - debug\n    environment:\n      <<: *airflow-common-env \n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n\n  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    <<: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - \"5555:5555\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      <<: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  git-sync:\n    image: databurst/git-sync:latest\n    ports:\n      - \"9418:9418\"\n    volumes:\n      - ./project_data:${DESTINATION_PATH:-/app/sync}\n    environment:\n      REPO_URL: https://github.com/karimulla0908/airflow.git\n      GIT_BRANCH: ${GIT_BRANCH:-main}\n      DIRECTORY_NAME: ${DIRECTORY_NAME:-project}\n      DESTINATION_PATH: ${DESTINATION_PATH:-/app/sync}\n      INTERVAL: ${INTERVAL:-10}\n\nvolumes:\n  postgres-db-volume:\n\n"
  userlist.txt: '"airflow" "md5$(echo -n airflowairflow | md5sum | awk ''{print $1}'')"'
kind: ConfigMap
metadata:
  labels:
    io.kompose.service: airflow-init
  name: airflow-init-cm0
